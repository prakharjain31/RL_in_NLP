{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/prakharjain/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will import our dataset of airline reviews which is taken from [here](https://www.kaggle.com/datasets/kanchana1990/singapore-airlines-reviews?resource=download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"singapore_airlines_reviews.csv\" , sep=\",\",on_bad_lines='warn', header=0,usecols=[\"rating\",\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check if the data is loaded properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>We used this airline to go from Singapore to L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The service on Singapore Airlines Suites Class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Booked, paid and received email confirmation f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Best airline in the world, seats, food, servic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Premium Economy Seating on Singapore Airlines ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                               text\n",
       "0       3  We used this airline to go from Singapore to L...\n",
       "1       5  The service on Singapore Airlines Suites Class...\n",
       "2       1  Booked, paid and received email confirmation f...\n",
       "3       5  Best airline in the world, seats, food, servic...\n",
       "4       2  Premium Economy Seating on Singapore Airlines ..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>We used this airline to go from Singapore to L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The service on Singapore Airlines Suites Class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Booked, paid and received email confirmation f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Best airline in the world, seats, food, servic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Premium Economy Seating on Singapore Airlines ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>5</td>\n",
       "      <td>First part done with Singapore Airlines - acce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>5</td>\n",
       "      <td>And again a great Flight with Singapore Air. G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>5</td>\n",
       "      <td>We flew business class from Frankfurt, via Sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>4</td>\n",
       "      <td>As always, the A380 aircraft was spotlessly pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>4</td>\n",
       "      <td>As always, Singapore Airlines has done it agai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating                                               text\n",
       "0          3  We used this airline to go from Singapore to L...\n",
       "1          5  The service on Singapore Airlines Suites Class...\n",
       "2          1  Booked, paid and received email confirmation f...\n",
       "3          5  Best airline in the world, seats, food, servic...\n",
       "4          2  Premium Economy Seating on Singapore Airlines ...\n",
       "...      ...                                                ...\n",
       "9995       5  First part done with Singapore Airlines - acce...\n",
       "9996       5  And again a great Flight with Singapore Air. G...\n",
       "9997       5  We flew business class from Frankfurt, via Sin...\n",
       "9998       4  As always, the A380 aircraft was spotlessly pr...\n",
       "9999       4  As always, Singapore Airlines has done it agai...\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    5424\n",
       "4    1967\n",
       "1    1057\n",
       "3    1009\n",
       "2     543\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>we used this airline to go from singapore to l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>the service on singapore airlines suites class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>booked, paid and received email confirmation f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>best airline in the world, seats, food, servic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>premium economy seating on singapore airlines ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>5</td>\n",
       "      <td>first part done with singapore airlines - acce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>5</td>\n",
       "      <td>and again a great flight with singapore air. g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>5</td>\n",
       "      <td>we flew business class from frankfurt, via sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>4</td>\n",
       "      <td>as always, the a380 aircraft was spotlessly pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>4</td>\n",
       "      <td>as always, singapore airlines has done it agai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating                                               text\n",
       "0          3  we used this airline to go from singapore to l...\n",
       "1          5  the service on singapore airlines suites class...\n",
       "2          1  booked, paid and received email confirmation f...\n",
       "3          5  best airline in the world, seats, food, servic...\n",
       "4          2  premium economy seating on singapore airlines ...\n",
       "...      ...                                                ...\n",
       "9995       5  first part done with singapore airlines - acce...\n",
       "9996       5  and again a great flight with singapore air. g...\n",
       "9997       5  we flew business class from frankfurt, via sin...\n",
       "9998       4  as always, the a380 aircraft was spotlessly pr...\n",
       "9999       4  as always, singapore airlines has done it agai...\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use nltk library to tokenise the data and and regex library to remove all non alpha characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>we used this airline to go from singapore to l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>the service on singapore airlines suites class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>booked  paid and received email confirmation f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>best airline in the world  seats  food  servic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>premium economy seating on singapore airlines ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>5</td>\n",
       "      <td>first part done with singapore airlines  accep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>5</td>\n",
       "      <td>and again a great flight with singapore air  g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>5</td>\n",
       "      <td>we flew business class from frankfurt  via sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>4</td>\n",
       "      <td>as always  the a aircraft was spotlessly prese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>4</td>\n",
       "      <td>as always  singapore airlines has done it agai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating                                               text\n",
       "0          3  we used this airline to go from singapore to l...\n",
       "1          5  the service on singapore airlines suites class...\n",
       "2          1  booked  paid and received email confirmation f...\n",
       "3          5  best airline in the world  seats  food  servic...\n",
       "4          2  premium economy seating on singapore airlines ...\n",
       "...      ...                                                ...\n",
       "9995       5  first part done with singapore airlines  accep...\n",
       "9996       5  and again a great flight with singapore air  g...\n",
       "9997       5  we flew business class from frankfurt  via sin...\n",
       "9998       4  as always  the a aircraft was spotlessly prese...\n",
       "9999       4  as always  singapore airlines has done it agai...\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(lambda x : \" \".join( [re.sub('[^A-Za-z]+','',x) for x in nltk.word_tokenize(x)] ))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: re.sub(' +',' ',x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform lemmatization. What it does is it groups together similar types of words to a same word hence increasing our data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>we used this airline to go from singapore to l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>the service on singapore airline suite class w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>booked paid and received email confirmation fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>best airline in the world seat food service ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>premium economy seating on singapore airline h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>5</td>\n",
       "      <td>first part done with singapore airline accepta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>5</td>\n",
       "      <td>and again a great flight with singapore air gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>5</td>\n",
       "      <td>we flew business class from frankfurt via sing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>4</td>\n",
       "      <td>a always the a aircraft wa spotlessly presente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>4</td>\n",
       "      <td>a always singapore airline ha done it again i ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating                                               text\n",
       "0          3  we used this airline to go from singapore to l...\n",
       "1          5  the service on singapore airline suite class w...\n",
       "2          1  booked paid and received email confirmation fo...\n",
       "3          5  best airline in the world seat food service ar...\n",
       "4          2  premium economy seating on singapore airline h...\n",
       "...      ...                                                ...\n",
       "9995       5  first part done with singapore airline accepta...\n",
       "9996       5  and again a great flight with singapore air gr...\n",
       "9997       5  we flew business class from frankfurt via sing...\n",
       "9998       4  a always the a aircraft wa spotlessly presente...\n",
       "9999       4  a always singapore airline ha done it again i ...\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['text'] = df['text'].apply(lambda x : \" \".join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(x)]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(df['text'] , df['rating'] , test_size=0.15,random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8500,) (8500,)\n",
      "(1500,) (1500,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape , Y_train.shape)\n",
    "print(X_test.shape , Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tf_X_train = vectorizer.fit_transform(X_train)\n",
    "tf_X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1064)\t0.1355984005192861\n",
      "  (0, 8475)\t0.10089088496782413\n",
      "  (0, 16132)\t0.11258651947733131\n",
      "  (0, 14901)\t0.0585054268736031\n",
      "  (0, 5074)\t0.10802553924294064\n",
      "  (0, 2029)\t0.25540842684662307\n",
      "  (0, 11116)\t0.1887264503100878\n",
      "  (0, 14719)\t0.07600744198502185\n",
      "  (0, 709)\t0.22541365632678084\n",
      "  (0, 11977)\t0.21755924050159015\n",
      "  (0, 870)\t0.06047844510892339\n",
      "  (0, 16681)\t0.12622627957771826\n",
      "  (0, 8931)\t0.0752345915664352\n",
      "  (0, 8399)\t0.08378548736399491\n",
      "  (0, 7161)\t0.07666536338018506\n",
      "  (0, 12923)\t0.04465752720798861\n",
      "  (0, 7452)\t0.22541365632678084\n",
      "  (0, 12212)\t0.19254228910252713\n",
      "  (0, 4106)\t0.23648383367410655\n",
      "  (0, 14732)\t0.09020390572452132\n",
      "  (0, 3630)\t0.22120497022833774\n",
      "  (0, 7617)\t0.10652337732282983\n",
      "  (0, 11412)\t0.17501812682836382\n",
      "  (0, 7269)\t0.08406877375031657\n",
      "  (0, 7566)\t0.23039147544755997\n",
      "  :\t:\n",
      "  (8499, 957)\t0.1934506941204232\n",
      "  (8499, 5080)\t0.1963074631837091\n",
      "  (8499, 5619)\t0.1387728585640198\n",
      "  (8499, 8729)\t0.2368313573203767\n",
      "  (8499, 2608)\t0.12495033769314863\n",
      "  (8499, 6414)\t0.12029320942358754\n",
      "  (8499, 3327)\t0.23764759068137875\n",
      "  (8499, 9450)\t0.10055779995328099\n",
      "  (8499, 13606)\t0.11272245723417174\n",
      "  (8499, 14698)\t0.10239773209937612\n",
      "  (8499, 9758)\t0.18971443299339374\n",
      "  (8499, 6530)\t0.10596535111511701\n",
      "  (8499, 9921)\t0.19064692763835134\n",
      "  (8499, 5879)\t0.16204390069665978\n",
      "  (8499, 14737)\t0.13074003702600015\n",
      "  (8499, 4014)\t0.14392638330176585\n",
      "  (8499, 8399)\t0.1498701489948798\n",
      "  (8499, 12923)\t0.07988054336102432\n",
      "  (8499, 13889)\t0.11381851121308129\n",
      "  (8499, 14708)\t0.16647609419357376\n",
      "  (8499, 5706)\t0.06988948143261922\n",
      "  (8499, 16078)\t0.27692351598086135\n",
      "  (8499, 674)\t0.11045879316807419\n",
      "  (8499, 9989)\t0.07812267526703633\n",
      "  (8499, 16203)\t0.19671047119010343\n"
     ]
    }
   ],
   "source": [
    "print(tf_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some popular learning models for learning languages are Support vector machines and neural networks, lets try and implement 3 models - a logistic regression model, an SVM and a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svcModel = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcModel.fit(tf_X_train , Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_svm_predictions = svcModel.predict(tf_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.6813186813186813,\n",
       "  'recall': 0.7848101265822784,\n",
       "  'f1-score': 0.7294117647058823,\n",
       "  'support': 158},\n",
       " '2': {'precision': 0.23529411764705882,\n",
       "  'recall': 0.1,\n",
       "  'f1-score': 0.14035087719298245,\n",
       "  'support': 80},\n",
       " '3': {'precision': 0.5,\n",
       "  'recall': 0.36875,\n",
       "  'f1-score': 0.4244604316546763,\n",
       "  'support': 160},\n",
       " '4': {'precision': 0.4232558139534884,\n",
       "  'recall': 0.30847457627118646,\n",
       "  'f1-score': 0.3568627450980392,\n",
       "  'support': 295},\n",
       " '5': {'precision': 0.7581493165089379,\n",
       "  'recall': 0.8934324659231723,\n",
       "  'f1-score': 0.820250284414107,\n",
       "  'support': 807},\n",
       " 'accuracy': 0.6686666666666666,\n",
       " 'macro avg': {'precision': 0.5196035858856333,\n",
       "  'recall': 0.49109343375532744,\n",
       "  'f1-score': 0.4942672206131375,\n",
       "  'support': 1500},\n",
       " 'weighted avg': {'precision': 0.6287725630660722,\n",
       "  'recall': 0.6686666666666666,\n",
       "  'f1-score': 0.6410701915932147,\n",
       "  'support': 1500}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "svm_report = classification_report(Y_test , Y_test_svm_predictions , output_dict=True)\n",
    "svm_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticModel = LogisticRegression(max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticModel.fit(tf_X_train , Y_train)\n",
    "Y_test_logistic_predictions = logisticModel.predict(tf_X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 5, ..., 5, 1, 5])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_logistic_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.727810650887574,\n",
       "  'recall': 0.7784810126582279,\n",
       "  'f1-score': 0.7522935779816514,\n",
       "  'support': 158},\n",
       " '2': {'precision': 0.2727272727272727,\n",
       "  'recall': 0.0375,\n",
       "  'f1-score': 0.06593406593406594,\n",
       "  'support': 80},\n",
       " '3': {'precision': 0.4956521739130435,\n",
       "  'recall': 0.35625,\n",
       "  'f1-score': 0.41454545454545455,\n",
       "  'support': 160},\n",
       " '4': {'precision': 0.43169398907103823,\n",
       "  'recall': 0.2677966101694915,\n",
       "  'f1-score': 0.3305439330543933,\n",
       "  'support': 295},\n",
       " '5': {'precision': 0.726027397260274,\n",
       "  'recall': 0.919454770755886,\n",
       "  'f1-score': 0.811372334609076,\n",
       "  'support': 807},\n",
       " 'accuracy': 0.6693333333333333,\n",
       " 'macro avg': {'precision': 0.5307822967718405,\n",
       "  'recall': 0.47189647871672113,\n",
       "  'f1-score': 0.47493787322492825,\n",
       "  'support': 1500},\n",
       " 'weighted avg': {'precision': 0.619580299233002,\n",
       "  'recall': 0.6693333333333333,\n",
       "  'f1-score': 0.6285015450691129,\n",
       "  'support': 1500}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_report = classification_report(Y_test , Y_test_logistic_predictions , output_dict=True)\n",
    "logistic_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8500, 16770)\n"
     ]
    }
   ],
   "source": [
    "print(tf_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "norm = Normalizer().fit(tf_X_train)\n",
    "tf_X_train_norm = norm.transform(tf_X_train)\n",
    "tf_X_test_norm = norm.transform(tf_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_nn_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,),\n",
    "    activation='relu',\n",
    "    max_iter=3000,\n",
    "    random_state=21\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 5, ..., 5, 2, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_nn_model.fit(tf_X_train , Y_train)\n",
    "new_nn_predictions = new_nn_model.predict(tf_X_test)\n",
    "new_nn_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.6851851851851852,\n",
       "  'recall': 0.7025316455696202,\n",
       "  'f1-score': 0.69375,\n",
       "  'support': 158},\n",
       " '2': {'precision': 0.2,\n",
       "  'recall': 0.125,\n",
       "  'f1-score': 0.15384615384615385,\n",
       "  'support': 80},\n",
       " '3': {'precision': 0.5,\n",
       "  'recall': 0.4125,\n",
       "  'f1-score': 0.4520547945205479,\n",
       "  'support': 160},\n",
       " '4': {'precision': 0.37362637362637363,\n",
       "  'recall': 0.34576271186440677,\n",
       "  'f1-score': 0.35915492957746475,\n",
       "  'support': 295},\n",
       " '5': {'precision': 0.7587768969422424,\n",
       "  'recall': 0.8302354399008675,\n",
       "  'f1-score': 0.7928994082840238,\n",
       "  'support': 807},\n",
       " 'accuracy': 0.6393333333333333,\n",
       " 'macro avg': {'precision': 0.5035176911507603,\n",
       "  'recall': 0.4832059594669788,\n",
       "  'f1-score': 0.4903410572456381,\n",
       "  'support': 1500},\n",
       " 'weighted avg': {'precision': 0.6178746635409528,\n",
       "  'recall': 0.6393333333333333,\n",
       "  'f1-score': 0.6267129907610262,\n",
       "  'support': 1500}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_nn_report = classification_report(Y_test , new_nn_predictions , output_dict=True)\n",
    "new_nn_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural net fails to outperform the logisitc regression or the svm, which is most probably due to the large number of features of the input data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8500, 16770)\n",
      "(8500, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "principal = TruncatedSVD(n_components=1000)\n",
    "print(tf_X_train.shape)\n",
    "principal.fit(tf_X_train)\n",
    "pca_x_train = principal.transform(tf_X_train)\n",
    "print(pca_x_train.shape)\n",
    "principal.fit(tf_X_test)\n",
    "pca_x_test = principal.transform(tf_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 5, ..., 5, 1, 5])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,),\n",
    "    activation='relu',\n",
    "    max_iter=3000,\n",
    "    random_state=21\n",
    ")\n",
    "nn_model.fit(pca_x_train , Y_train)\n",
    "new2_nn_predictions = nn_model.predict(pca_x_test)\n",
    "new2_nn_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.2641509433962264,\n",
       "  'recall': 0.26582278481012656,\n",
       "  'f1-score': 0.2649842271293375,\n",
       "  'support': 158},\n",
       " '2': {'precision': 0.07766990291262135,\n",
       "  'recall': 0.1,\n",
       "  'f1-score': 0.08743169398907104,\n",
       "  'support': 80},\n",
       " '3': {'precision': 0.1656441717791411,\n",
       "  'recall': 0.16875,\n",
       "  'f1-score': 0.16718266253869968,\n",
       "  'support': 160},\n",
       " '4': {'precision': 0.23,\n",
       "  'recall': 0.23389830508474577,\n",
       "  'f1-score': 0.2319327731092437,\n",
       "  'support': 295},\n",
       " '5': {'precision': 0.6438709677419355,\n",
       "  'recall': 0.6183395291201983,\n",
       "  'f1-score': 0.6308470290771178,\n",
       "  'support': 807},\n",
       " 'accuracy': 0.43,\n",
       " 'macro avg': {'precision': 0.27626719716598486,\n",
       "  'recall': 0.27736212380301406,\n",
       "  'f1-score': 0.2764756771686939,\n",
       "  'support': 1500},\n",
       " 'weighted avg': {'precision': 0.4412709198280121,\n",
       "  'recall': 0.43,\n",
       "  'f1-score': 0.43541665996280926,\n",
       "  'support': 1500}}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new2_nn_report = classification_report(Y_test , new2_nn_predictions , output_dict=True)\n",
    "new2_nn_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try word2vec, another method to encode our text into some numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "w2v_X_train = X_train.copy()\n",
    "w2v_X_test = X_test.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_X_train = [simple_preprocess(line , deacc=True) for line in w2v_X_train]\n",
    "w2v_X_test = [simple_preprocess(line , deacc=True) for line in w2v_X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['we',\n",
       "  'recent',\n",
       "  'travel',\n",
       "  'to',\n",
       "  'oz',\n",
       "  'on',\n",
       "  'holidai',\n",
       "  'and',\n",
       "  'although',\n",
       "  'it',\n",
       "  'wa',\n",
       "  'good',\n",
       "  'comfort',\n",
       "  'flight',\n",
       "  'with',\n",
       "  'decent',\n",
       "  'food',\n",
       "  'the',\n",
       "  'cabin',\n",
       "  'staff',\n",
       "  'were',\n",
       "  'veri',\n",
       "  'thin',\n",
       "  'on',\n",
       "  'the',\n",
       "  'ground',\n",
       "  'and',\n",
       "  'even',\n",
       "  'although',\n",
       "  'most',\n",
       "  'of',\n",
       "  'the',\n",
       "  'flight',\n",
       "  'wa',\n",
       "  'made',\n",
       "  'dure',\n",
       "  'the',\n",
       "  'dai',\n",
       "  'thei',\n",
       "  'seem',\n",
       "  'intent',\n",
       "  'in',\n",
       "  'put',\n",
       "  'the',\n",
       "  'cabin',\n",
       "  'into',\n",
       "  'dark',\n",
       "  'and',\n",
       "  'then',\n",
       "  'disappear',\n",
       "  'result',\n",
       "  'in',\n",
       "  'infrequ',\n",
       "  'servic',\n",
       "  'if',\n",
       "  'like',\n",
       "  'me',\n",
       "  'you',\n",
       "  'ar',\n",
       "  'reluct',\n",
       "  'to',\n",
       "  'annoi',\n",
       "  'them',\n",
       "  'and',\n",
       "  'press',\n",
       "  'the',\n",
       "  'buzzer',\n",
       "  'everi',\n",
       "  'time',\n",
       "  'you',\n",
       "  'want',\n",
       "  'littl',\n",
       "  'attent'],\n",
       " ['singapor',\n",
       "  'airlin',\n",
       "  'ar',\n",
       "  'consist',\n",
       "  'good',\n",
       "  'with',\n",
       "  'their',\n",
       "  'servic',\n",
       "  'onboard',\n",
       "  'entertain',\n",
       "  'and',\n",
       "  'food',\n",
       "  'it',\n",
       "  'wa',\n",
       "  'veri',\n",
       "  'long',\n",
       "  'flight',\n",
       "  'and',\n",
       "  'thei',\n",
       "  'did',\n",
       "  'nt',\n",
       "  'disappoint',\n",
       "  'there',\n",
       "  'were',\n",
       "  'few',\n",
       "  'occa',\n",
       "  'when',\n",
       "  'for',\n",
       "  'some',\n",
       "  'inexpl',\n",
       "  'reason',\n",
       "  'wa',\n",
       "  'bypass',\n",
       "  'when',\n",
       "  'drink',\n",
       "  'were',\n",
       "  'be',\n",
       "  'offer',\n",
       "  'otherwi',\n",
       "  'thei',\n",
       "  'made',\n",
       "  'long',\n",
       "  'tough',\n",
       "  'journei',\n",
       "  'bearabl'],\n",
       " ['the',\n",
       "  'trip',\n",
       "  'over',\n",
       "  'which',\n",
       "  'thi',\n",
       "  'wa',\n",
       "  'wa',\n",
       "  'on',\n",
       "  'an',\n",
       "  'older',\n",
       "  'aircraft',\n",
       "  'old',\n",
       "  'airbu',\n",
       "  'and',\n",
       "  'the',\n",
       "  'seat',\n",
       "  'wa',\n",
       "  'poor',\n",
       "  'and',\n",
       "  'servic',\n",
       "  'mediocr',\n",
       "  'the',\n",
       "  'return',\n",
       "  'trip',\n",
       "  'from',\n",
       "  'london',\n",
       "  'to',\n",
       "  'melbourn',\n",
       "  'wa',\n",
       "  'veri',\n",
       "  'good',\n",
       "  'both',\n",
       "  'in',\n",
       "  'seat',\n",
       "  'and',\n",
       "  'servic'],\n",
       " ['have',\n",
       "  'flown',\n",
       "  'from',\n",
       "  'manchest',\n",
       "  'to',\n",
       "  'perth',\n",
       "  'with',\n",
       "  'on',\n",
       "  'stop',\n",
       "  'in',\n",
       "  'singapor',\n",
       "  'where',\n",
       "  'the',\n",
       "  'flight',\n",
       "  'went',\n",
       "  'well',\n",
       "  'on',\n",
       "  'time',\n",
       "  'with',\n",
       "  'the',\n",
       "  'high',\n",
       "  'level',\n",
       "  'of',\n",
       "  'comfort',\n",
       "  'servic',\n",
       "  'food',\n",
       "  'and',\n",
       "  'drink',\n",
       "  'hot',\n",
       "  'towel',\n",
       "  'inflight',\n",
       "  'entertain',\n",
       "  'etc',\n",
       "  'we',\n",
       "  'decid',\n",
       "  'on',\n",
       "  'the',\n",
       "  'return',\n",
       "  'fight',\n",
       "  'to',\n",
       "  'stop',\n",
       "  'at',\n",
       "  'both',\n",
       "  'singapor',\n",
       "  'and',\n",
       "  'pari',\n",
       "  'although',\n",
       "  'we',\n",
       "  'check',\n",
       "  'in',\n",
       "  'onlin',\n",
       "  'for',\n",
       "  'the',\n",
       "  'outward',\n",
       "  'flight',\n",
       "  'we',\n",
       "  'could',\n",
       "  'nt',\n",
       "  'do',\n",
       "  'it',\n",
       "  'for',\n",
       "  'the',\n",
       "  'return',\n",
       "  'flight',\n",
       "  'due',\n",
       "  'to',\n",
       "  'the',\n",
       "  'pari',\n",
       "  'to',\n",
       "  'manchest',\n",
       "  'section',\n",
       "  'it',\n",
       "  'did',\n",
       "  'nt',\n",
       "  'cau',\n",
       "  'problem',\n",
       "  'we',\n",
       "  'just',\n",
       "  'check',\n",
       "  'our',\n",
       "  'luggag',\n",
       "  'all',\n",
       "  'the',\n",
       "  'wai',\n",
       "  'through',\n",
       "  'at',\n",
       "  'perth',\n",
       "  'although',\n",
       "  'the',\n",
       "  'board',\n",
       "  'card',\n",
       "  'for',\n",
       "  'the',\n",
       "  'pari',\n",
       "  'to',\n",
       "  'manchest',\n",
       "  'section',\n",
       "  'had',\n",
       "  'to',\n",
       "  'be',\n",
       "  'pick',\n",
       "  'up',\n",
       "  'at',\n",
       "  'the',\n",
       "  'singapor',\n",
       "  'airlin',\n",
       "  'desk',\n",
       "  'in',\n",
       "  'singapor',\n",
       "  'again',\n",
       "  'not',\n",
       "  'problem',\n",
       "  'the',\n",
       "  'troubl',\n",
       "  'start',\n",
       "  'when',\n",
       "  'we',\n",
       "  'land',\n",
       "  'at',\n",
       "  'pari',\n",
       "  'we',\n",
       "  'arriv',\n",
       "  'slightli',\n",
       "  'late',\n",
       "  'and',\n",
       "  'the',\n",
       "  'turnaround',\n",
       "  'time',\n",
       "  'to',\n",
       "  'the',\n",
       "  'last',\n",
       "  'flight',\n",
       "  'flybe',\n",
       "  'under',\n",
       "  'singapor',\n",
       "  'airlin',\n",
       "  'wa',\n",
       "  'veri',\n",
       "  'tight',\n",
       "  'to',\n",
       "  'ensur',\n",
       "  'we',\n",
       "  'did',\n",
       "  'nt',\n",
       "  'miss',\n",
       "  'the',\n",
       "  'flight',\n",
       "  'singapor',\n",
       "  'airlin',\n",
       "  'ensur',\n",
       "  'we',\n",
       "  'quickli',\n",
       "  'went',\n",
       "  'through',\n",
       "  'person',\n",
       "  'secur',\n",
       "  'check',\n",
       "  'etc',\n",
       "  'and',\n",
       "  'onto',\n",
       "  'the',\n",
       "  'bu',\n",
       "  'to',\n",
       "  'the',\n",
       "  'correct',\n",
       "  'termin',\n",
       "  'we',\n",
       "  'arriv',\n",
       "  'and',\n",
       "  'although',\n",
       "  'singapor',\n",
       "  'airlin',\n",
       "  'had',\n",
       "  'confirm',\n",
       "  'that',\n",
       "  'we',\n",
       "  'were',\n",
       "  'ok',\n",
       "  'to',\n",
       "  'come',\n",
       "  'through',\n",
       "  'the',\n",
       "  'french',\n",
       "  'in',\n",
       "  'their',\n",
       "  'usual',\n",
       "  'fashion',\n",
       "  'said',\n",
       "  'non',\n",
       "  'it',\n",
       "  'wa',\n",
       "  'nt',\n",
       "  'help',\n",
       "  'by',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'no',\n",
       "  'flybe',\n",
       "  'repr',\n",
       "  'wa',\n",
       "  'contact',\n",
       "  'the',\n",
       "  'repr',\n",
       "  'persev',\n",
       "  'while',\n",
       "  'we',\n",
       "  'had',\n",
       "  'bu',\n",
       "  'tour',\n",
       "  'around',\n",
       "  'the',\n",
       "  'airport',\n",
       "  'in',\n",
       "  'the',\n",
       "  'end',\n",
       "  'we',\n",
       "  'were',\n",
       "  'allow',\n",
       "  'through',\n",
       "  'onto',\n",
       "  'our',\n",
       "  'flight',\n",
       "  'leav',\n",
       "  'minut',\n",
       "  'late',\n",
       "  'there',\n",
       "  'is',\n",
       "  'alwai',\n",
       "  'sting',\n",
       "  'in',\n",
       "  'the',\n",
       "  'tail',\n",
       "  'where',\n",
       "  'the',\n",
       "  'french',\n",
       "  'ar',\n",
       "  'concern',\n",
       "  'and',\n",
       "  'it',\n",
       "  'came',\n",
       "  'when',\n",
       "  'we',\n",
       "  'tri',\n",
       "  'to',\n",
       "  'collect',\n",
       "  'our',\n",
       "  'luggag',\n",
       "  'amazingli',\n",
       "  'it',\n",
       "  'wa',\n",
       "  'still',\n",
       "  'in',\n",
       "  'pari',\n",
       "  'talk',\n",
       "  'to',\n",
       "  'other',\n",
       "  'passeng',\n",
       "  'who',\n",
       "  'had',\n",
       "  'flown',\n",
       "  'into',\n",
       "  'pari',\n",
       "  'with',\n",
       "  'it',\n",
       "  'appear',\n",
       "  'thi',\n",
       "  'is',\n",
       "  'not',\n",
       "  'uniqu',\n",
       "  'case',\n",
       "  'suggest',\n",
       "  'avoid',\n",
       "  'pari',\n",
       "  'and',\n",
       "  'us',\n",
       "  'amsterdam',\n",
       "  'or',\n",
       "  'dusseldorf',\n",
       "  'no',\n",
       "  'problem',\n",
       "  'with',\n",
       "  'singapor',\n",
       "  'airlin',\n",
       "  'but',\n",
       "  'the',\n",
       "  'french',\n",
       "  'and',\n",
       "  'flybe',\n",
       "  'ar',\n",
       "  'differ',\n",
       "  'stori'],\n",
       " ['the',\n",
       "  'plane',\n",
       "  'wa',\n",
       "  'old',\n",
       "  'the',\n",
       "  'seat',\n",
       "  'were',\n",
       "  'dirti',\n",
       "  'and',\n",
       "  'the',\n",
       "  'trai',\n",
       "  'tabl',\n",
       "  'wa',\n",
       "  'wor',\n",
       "  'the',\n",
       "  'staff',\n",
       "  'did',\n",
       "  'noth',\n",
       "  'except',\n",
       "  'so',\n",
       "  'the',\n",
       "  'onli',\n",
       "  'good',\n",
       "  'thing',\n",
       "  'wa',\n",
       "  'that',\n",
       "  'it',\n",
       "  'left',\n",
       "  'in',\n",
       "  'time'],\n",
       " ['my',\n",
       "  'famili',\n",
       "  'and',\n",
       "  'wa',\n",
       "  'suppo',\n",
       "  'to',\n",
       "  'fly',\n",
       "  'back',\n",
       "  'to',\n",
       "  'new',\n",
       "  'zealand',\n",
       "  'but',\n",
       "  'due',\n",
       "  'to',\n",
       "  'suspect',\n",
       "  'fire',\n",
       "  'in',\n",
       "  'singapor',\n",
       "  'the',\n",
       "  'flight',\n",
       "  'wa',\n",
       "  'delai',\n",
       "  'the',\n",
       "  'staff',\n",
       "  'of',\n",
       "  'the',\n",
       "  'airlin',\n",
       "  'wa',\n",
       "  'brilliant',\n",
       "  'we',\n",
       "  'receiv',\n",
       "  'restaur',\n",
       "  'voucher',\n",
       "  'for',\n",
       "  'oliv',\n",
       "  'tambo',\n",
       "  'intern',\n",
       "  'airport',\n",
       "  'we',\n",
       "  'receiv',\n",
       "  'taxi',\n",
       "  'ride',\n",
       "  'to',\n",
       "  'hotel',\n",
       "  'in',\n",
       "  'singapor',\n",
       "  'bedroom',\n",
       "  'for',\n",
       "  'the',\n",
       "  'of',\n",
       "  'gourmet',\n",
       "  'breakfast',\n",
       "  'and',\n",
       "  'lunch',\n",
       "  'thei',\n",
       "  'were',\n",
       "  'simpli',\n",
       "  'outstand',\n",
       "  'in',\n",
       "  'provid',\n",
       "  'assur',\n",
       "  'and',\n",
       "  'comfort'],\n",
       " ['did',\n",
       "  'not',\n",
       "  'want',\n",
       "  'to',\n",
       "  'stai',\n",
       "  'in',\n",
       "  'langkawi',\n",
       "  'for',\n",
       "  'long',\n",
       "  'time',\n",
       "  'so',\n",
       "  'took',\n",
       "  'flight',\n",
       "  'to',\n",
       "  'kl',\n",
       "  'think',\n",
       "  'can',\n",
       "  'catch',\n",
       "  'my',\n",
       "  'flight',\n",
       "  'from',\n",
       "  'kl',\n",
       "  'instead',\n",
       "  'of',\n",
       "  'lgk',\n",
       "  'call',\n",
       "  'and',\n",
       "  'the',\n",
       "  'rep',\n",
       "  'said',\n",
       "  'thei',\n",
       "  'can',\n",
       "  'not',\n",
       "  'chang',\n",
       "  'it',\n",
       "  'told',\n",
       "  'thei',\n",
       "  'can',\n",
       "  'put',\n",
       "  'note',\n",
       "  'sai',\n",
       "  'we',\n",
       "  'will',\n",
       "  'board',\n",
       "  'in',\n",
       "  'kl',\n",
       "  'lgk',\n",
       "  'the',\n",
       "  'rep',\n",
       "  'kept',\n",
       "  'me',\n",
       "  'on',\n",
       "  'the',\n",
       "  'line',\n",
       "  'for',\n",
       "  'hour',\n",
       "  'minut',\n",
       "  'and',\n",
       "  'did',\n",
       "  'noth',\n",
       "  'tri',\n",
       "  'talk',\n",
       "  'to',\n",
       "  'the',\n",
       "  'supervisor',\n",
       "  'but',\n",
       "  'thei',\n",
       "  'never',\n",
       "  'came',\n",
       "  'on',\n",
       "  'line',\n",
       "  'am',\n",
       "  'veri',\n",
       "  'veri',\n",
       "  'disappoint',\n",
       "  'with',\n",
       "  'singapor',\n",
       "  'airlin',\n",
       "  'had',\n",
       "  'to',\n",
       "  'call',\n",
       "  'my',\n",
       "  'travel',\n",
       "  'agent',\n",
       "  'and',\n",
       "  'thei',\n",
       "  'had',\n",
       "  'to',\n",
       "  'cancel',\n",
       "  'thi',\n",
       "  'trip',\n",
       "  'and',\n",
       "  'book',\n",
       "  'new',\n",
       "  'it',\n",
       "  'cost',\n",
       "  'extra',\n",
       "  'wast',\n",
       "  'monei',\n",
       "  'becau',\n",
       "  'of',\n",
       "  'singapor',\n",
       "  'airlin',\n",
       "  'you',\n",
       "  'gui',\n",
       "  'do',\n",
       "  'not',\n",
       "  'think',\n",
       "  'about',\n",
       "  'the',\n",
       "  'custom',\n",
       "  'but',\n",
       "  'just',\n",
       "  'about',\n",
       "  'yourself',\n",
       "  'your',\n",
       "  'rep',\n",
       "  'just',\n",
       "  'made',\n",
       "  'me',\n",
       "  'spend',\n",
       "  'extra',\n",
       "  'monei',\n",
       "  'for',\n",
       "  'noth',\n",
       "  'previou',\n",
       "  'airlin',\n",
       "  'have',\n",
       "  'place',\n",
       "  'note',\n",
       "  'in',\n",
       "  'the',\n",
       "  'itinerari',\n",
       "  'so',\n",
       "  'the',\n",
       "  'attend',\n",
       "  'know',\n",
       "  'in',\n",
       "  'similar',\n",
       "  'situat',\n",
       "  'am',\n",
       "  'from',\n",
       "  'and',\n",
       "  'have',\n",
       "  'larg',\n",
       "  'famili',\n",
       "  'and',\n",
       "  'friend',\n",
       "  'and',\n",
       "  'will',\n",
       "  'tell',\n",
       "  'everyon',\n",
       "  'about',\n",
       "  'thi',\n",
       "  'experi',\n",
       "  'pretti',\n",
       "  'sure',\n",
       "  'thei',\n",
       "  'would',\n",
       "  'want',\n",
       "  'to',\n",
       "  'go',\n",
       "  'through',\n",
       "  'the',\n",
       "  'same',\n",
       "  'experi',\n",
       "  'and',\n",
       "  'expen',\n",
       "  'had',\n",
       "  'heard',\n",
       "  'good',\n",
       "  'thing',\n",
       "  'about',\n",
       "  'singapor',\n",
       "  'airlin',\n",
       "  'but',\n",
       "  'my',\n",
       "  'experi',\n",
       "  'is',\n",
       "  'someth',\n",
       "  'differ',\n",
       "  'will',\n",
       "  'also',\n",
       "  'be',\n",
       "  'post',\n",
       "  'the',\n",
       "  'abov',\n",
       "  'in',\n",
       "  'all',\n",
       "  'the',\n",
       "  'travel',\n",
       "  'forum',\n",
       "  'too',\n",
       "  'so',\n",
       "  'passeng',\n",
       "  'do',\n",
       "  'not',\n",
       "  'have',\n",
       "  'spend',\n",
       "  'extra',\n",
       "  'monei',\n",
       "  'and',\n",
       "  'hassl',\n",
       "  'have',\n",
       "  'will',\n",
       "  'warn',\n",
       "  'mani',\n",
       "  'peopl',\n",
       "  'can',\n",
       "  'simpl',\n",
       "  'cancel',\n",
       "  'of',\n",
       "  'on',\n",
       "  'leg',\n",
       "  'of',\n",
       "  'flight',\n",
       "  'could',\n",
       "  'have',\n",
       "  'save',\n",
       "  'me',\n",
       "  'monei',\n",
       "  'and',\n",
       "  'so',\n",
       "  'much',\n",
       "  'hassl',\n",
       "  'weird',\n",
       "  'part',\n",
       "  'is',\n",
       "  'the',\n",
       "  'custom',\n",
       "  'servic',\n",
       "  'kept',\n",
       "  'me',\n",
       "  'on',\n",
       "  'the',\n",
       "  'phone',\n",
       "  'for',\n",
       "  'minut',\n",
       "  'he',\n",
       "  'come',\n",
       "  'on',\n",
       "  'the',\n",
       "  'line',\n",
       "  'and',\n",
       "  'sai',\n",
       "  'can',\n",
       "  'you',\n",
       "  'wait',\n",
       "  'for',\n",
       "  'coupl',\n",
       "  'of',\n",
       "  'minut',\n",
       "  'and',\n",
       "  'is',\n",
       "  'gone',\n",
       "  'for',\n",
       "  'thi',\n",
       "  'situat',\n",
       "  'wast',\n",
       "  'your',\n",
       "  'compani',\n",
       "  'monei',\n",
       "  'and',\n",
       "  'mine',\n",
       "  'who',\n",
       "  'ever',\n",
       "  'wrote',\n",
       "  'your',\n",
       "  'book',\n",
       "  'softwar',\n",
       "  'just',\n",
       "  'cost',\n",
       "  'you',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'monei',\n",
       "  'do',\n",
       "  'not',\n",
       "  'know',\n",
       "  'if',\n",
       "  'your',\n",
       "  'compani',\n",
       "  'will',\n",
       "  'do',\n",
       "  'anyth',\n",
       "  'about',\n",
       "  'thi',\n",
       "  'but',\n",
       "  'will',\n",
       "  'sure',\n",
       "  'make',\n",
       "  'sure',\n",
       "  'other',\n",
       "  'do',\n",
       "  'not',\n",
       "  'suffer',\n",
       "  'have',\n",
       "  'can',\n",
       "  'you',\n",
       "  'imagin',\n",
       "  'how',\n",
       "  'irrit',\n",
       "  'it',\n",
       "  'is',\n",
       "  'to',\n",
       "  'be',\n",
       "  'on',\n",
       "  'the',\n",
       "  'phone',\n",
       "  'for',\n",
       "  'minut',\n",
       "  'and',\n",
       "  'you',\n",
       "  'achiev',\n",
       "  'noth',\n",
       "  'total',\n",
       "  'ridicul'],\n",
       " ['thi',\n",
       "  'is',\n",
       "  'my',\n",
       "  'airlin',\n",
       "  'of',\n",
       "  'choic',\n",
       "  'becau',\n",
       "  'of',\n",
       "  'it',\n",
       "  'pure',\n",
       "  'servic',\n",
       "  'with',\n",
       "  'smile',\n",
       "  'from',\n",
       "  'cleanli',\n",
       "  'to',\n",
       "  'the',\n",
       "  'amen',\n",
       "  'food',\n",
       "  'is',\n",
       "  'just',\n",
       "  'normal',\n",
       "  'the',\n",
       "  'warm',\n",
       "  'welcom',\n",
       "  'make',\n",
       "  'me',\n",
       "  'feel',\n",
       "  'comfort',\n",
       "  'howev',\n",
       "  'bit',\n",
       "  'of',\n",
       "  'disappoint',\n",
       "  'due',\n",
       "  'to',\n",
       "  'my',\n",
       "  'seat',\n",
       "  'request',\n",
       "  'though',\n",
       "  'it',\n",
       "  'wa',\n",
       "  'grant',\n",
       "  'but',\n",
       "  'wa',\n",
       "  'at',\n",
       "  'the',\n",
       "  'last',\n",
       "  'row',\n",
       "  'in',\n",
       "  'the',\n",
       "  'tail',\n",
       "  'side',\n",
       "  'which',\n",
       "  'make',\n",
       "  'me',\n",
       "  'feel',\n",
       "  'groggi',\n",
       "  'throughout',\n",
       "  'the',\n",
       "  'flight'],\n",
       " ['wa',\n",
       "  'on',\n",
       "  'behalf',\n",
       "  'purcha',\n",
       "  'flight',\n",
       "  'ticket',\n",
       "  'sq',\n",
       "  'for',\n",
       "  'my',\n",
       "  'friend',\n",
       "  'on',\n",
       "  'nd',\n",
       "  'dec',\n",
       "  'on',\n",
       "  'wai',\n",
       "  'back',\n",
       "  'to',\n",
       "  'china',\n",
       "  'and',\n",
       "  'payment',\n",
       "  'by',\n",
       "  'my',\n",
       "  'credit',\n",
       "  'card',\n",
       "  'and',\n",
       "  'payment',\n",
       "  'have',\n",
       "  'be',\n",
       "  'declar',\n",
       "  'under',\n",
       "  'payment',\n",
       "  'refer',\n",
       "  'under',\n",
       "  'refer',\n",
       "  'number',\n",
       "  'given',\n",
       "  'by',\n",
       "  'db',\n",
       "  'bank',\n",
       "  'and',\n",
       "  'in',\n",
       "  'th',\n",
       "  'departur',\n",
       "  'my',\n",
       "  'friend',\n",
       "  'ca',\n",
       "  'nt',\n",
       "  'get',\n",
       "  'ani',\n",
       "  'ticket',\n",
       "  'from',\n",
       "  'counter',\n",
       "  'staff',\n",
       "  'yet',\n",
       "  'counter',\n",
       "  'staff',\n",
       "  'sai',\n",
       "  'my',\n",
       "  'credit',\n",
       "  'card',\n",
       "  'have',\n",
       "  'problem',\n",
       "  'and',\n",
       "  'ca',\n",
       "  'nt',\n",
       "  'deduct',\n",
       "  'the',\n",
       "  'amount',\n",
       "  'want',\n",
       "  'him',\n",
       "  'to',\n",
       "  'repurcha',\n",
       "  'air',\n",
       "  'ticket',\n",
       "  'again',\n",
       "  'ca',\n",
       "  'nt',\n",
       "  'let',\n",
       "  'him',\n",
       "  'on',\n",
       "  'board',\n",
       "  'my',\n",
       "  'friend',\n",
       "  'call',\n",
       "  'me',\n",
       "  'and',\n",
       "  'let',\n",
       "  'me',\n",
       "  'know',\n",
       "  'for',\n",
       "  'thi',\n",
       "  'issu',\n",
       "  'immedi',\n",
       "  'call',\n",
       "  'db',\n",
       "  'bank',\n",
       "  'and',\n",
       "  'check',\n",
       "  'on',\n",
       "  'my',\n",
       "  'card',\n",
       "  'bank',\n",
       "  'staff',\n",
       "  'have',\n",
       "  'credifi',\n",
       "  'and',\n",
       "  'sai',\n",
       "  'my',\n",
       "  'card',\n",
       "  'have',\n",
       "  'no',\n",
       "  'problem',\n",
       "  'and',\n",
       "  'payment',\n",
       "  'have',\n",
       "  'been',\n",
       "  'deduct',\n",
       "  'and',\n",
       "  'even',\n",
       "  'give',\n",
       "  'me',\n",
       "  'payment',\n",
       "  'refer',\n",
       "  'number',\n",
       "  'after',\n",
       "  'the',\n",
       "  'call',\n",
       "  'my',\n",
       "  'friend',\n",
       "  'want',\n",
       "  'to',\n",
       "  'show',\n",
       "  'on',\n",
       "  'the',\n",
       "  'payment',\n",
       "  'refer',\n",
       "  'number',\n",
       "  'yet',\n",
       "  'the',\n",
       "  'staff',\n",
       "  'have',\n",
       "  'ignor',\n",
       "  'my',\n",
       "  'friend',\n",
       "  'even',\n",
       "  'want',\n",
       "  'to',\n",
       "  'speak',\n",
       "  'to',\n",
       "  'the',\n",
       "  'staff',\n",
       "  'the',\n",
       "  'staff',\n",
       "  'also',\n",
       "  'ignor',\n",
       "  'me',\n",
       "  'now',\n",
       "  'need',\n",
       "  'to',\n",
       "  'repurcha',\n",
       "  'flight',\n",
       "  'ticket',\n",
       "  'again',\n",
       "  'veri',\n",
       "  'disappoint',\n",
       "  'singapor',\n",
       "  'airlin',\n",
       "  'staff',\n",
       "  'got',\n",
       "  'thi',\n",
       "  'kind',\n",
       "  'of',\n",
       "  'attitud',\n",
       "  'now',\n",
       "  'requir',\n",
       "  'for',\n",
       "  'the',\n",
       "  'refund',\n",
       "  'and',\n",
       "  'singapor',\n",
       "  'airlin',\n",
       "  'ignor',\n",
       "  'on',\n",
       "  'refund',\n",
       "  'matter',\n",
       "  'singaporean',\n",
       "  'feel',\n",
       "  'veri',\n",
       "  'shame',\n",
       "  'on',\n",
       "  'singapor',\n",
       "  'airlin',\n",
       "  'servic'],\n",
       " ['excel',\n",
       "  'servic',\n",
       "  'from',\n",
       "  'the',\n",
       "  'singapor',\n",
       "  'cabin',\n",
       "  'crew',\n",
       "  'from',\n",
       "  'manchest',\n",
       "  'to',\n",
       "  'singapor',\n",
       "  'and',\n",
       "  'from',\n",
       "  'new',\n",
       "  'zealand',\n",
       "  'cabin',\n",
       "  'crew',\n",
       "  'from',\n",
       "  'singapor',\n",
       "  'to',\n",
       "  'auckland',\n",
       "  'the',\n",
       "  'ground',\n",
       "  'staff',\n",
       "  'in',\n",
       "  'singapor',\n",
       "  'airport',\n",
       "  'were',\n",
       "  'veri',\n",
       "  'friendli',\n",
       "  'and',\n",
       "  'help',\n",
       "  'the',\n",
       "  'flight',\n",
       "  'wa',\n",
       "  'met',\n",
       "  'by',\n",
       "  'an',\n",
       "  'attend',\n",
       "  'to',\n",
       "  'direct',\n",
       "  'to',\n",
       "  'the',\n",
       "  'correct',\n",
       "  'gate',\n",
       "  'for',\n",
       "  'the',\n",
       "  'transfer',\n",
       "  'and',\n",
       "  'the',\n",
       "  'custom',\n",
       "  'staff',\n",
       "  'were',\n",
       "  'thorough',\n",
       "  'polit',\n",
       "  'and',\n",
       "  'friendli',\n",
       "  'to',\n",
       "  'get',\n",
       "  'through',\n",
       "  'the',\n",
       "  'check',\n",
       "  'quick',\n",
       "  'possibl',\n",
       "  'we',\n",
       "  'were',\n",
       "  'delai',\n",
       "  'the',\n",
       "  'movi',\n",
       "  'select',\n",
       "  'wa',\n",
       "  'excel',\n",
       "  'and',\n",
       "  'so',\n",
       "  'wa',\n",
       "  'the',\n",
       "  'choic',\n",
       "  'of',\n",
       "  'music',\n",
       "  'wa',\n",
       "  'happi',\n",
       "  'and',\n",
       "  'relax',\n",
       "  'listen',\n",
       "  'to',\n",
       "  'the',\n",
       "  'jazz',\n",
       "  'music',\n",
       "  'which',\n",
       "  'promot',\n",
       "  'rest',\n",
       "  'sleep',\n",
       "  'the',\n",
       "  'food',\n",
       "  'wa',\n",
       "  'veri',\n",
       "  'fresh',\n",
       "  'and',\n",
       "  'healthi',\n",
       "  'cater',\n",
       "  'for',\n",
       "  'special',\n",
       "  'diet',\n",
       "  'of',\n",
       "  'vegetarian',\n",
       "  'vegan',\n",
       "  'and',\n",
       "  'gluten',\n",
       "  'free',\n",
       "  'enjoi',\n",
       "  'gluten',\n",
       "  'free',\n",
       "  'health',\n",
       "  'snack',\n",
       "  'my',\n",
       "  'favourit',\n",
       "  'soi',\n",
       "  'yoghurt',\n",
       "  'and',\n",
       "  'veri',\n",
       "  'nice',\n",
       "  'wine',\n",
       "  'the',\n",
       "  'check',\n",
       "  'luggag',\n",
       "  'servic',\n",
       "  'from',\n",
       "  'manchest',\n",
       "  'to',\n",
       "  'auckland',\n",
       "  'wa',\n",
       "  'veri',\n",
       "  'good',\n",
       "  'my',\n",
       "  'luggag',\n",
       "  'wa',\n",
       "  'on',\n",
       "  'the',\n",
       "  'carousel',\n",
       "  'in',\n",
       "  'coupl',\n",
       "  'of',\n",
       "  'minut',\n",
       "  'wait',\n",
       "  'time']]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "porterStemmer = PorterStemmer()\n",
    "w2v_X_test = [[porterStemmer.stem(word) for word in tokens] for tokens in w2v_X_test ]\n",
    "w2v_X_train = [[porterStemmer.stem(word) for word in tokens] for tokens in w2v_X_train ]\n",
    "w2v_X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "stemmed_tokens = pd.Series(w2v_X_train).values\n",
    "\n",
    "size = 1000\n",
    "min_count = 3\n",
    "workers = 6\n",
    "sg = 1\n",
    "w2vModel = Word2Vec(stemmed_tokens , vector_size=size,min_count=min_count,sg=sg,workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vModel.save('w2v_1000.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_w2v_model = Word2Vec.load('w2v_1000.model')\n",
    "# print(len(sg_w2v_model.wv.vocab))\n",
    "with open('airline_review_w2v.csv' , 'w+') as w2v_file:\n",
    "    words = list(w for w in sg_w2v_model.wv.index_to_key)\n",
    "    header = \",\".join(str(ele) for ele in range(size))\n",
    "    w2v_file.write(header)\n",
    "    w2v_file.write(\"\\n\")\n",
    "    for row in w2v_X_train:\n",
    "        for token in row:\n",
    "            if(token in words):\n",
    "                model_vector = (np.mean([sg_w2v_model.wv[token]], axis=0)).tolist()\n",
    "        line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
    "        if(len(line1) == 0):\n",
    "            line1 = \",\".join([str(0) for i in range(size)])\n",
    "        w2v_file.write(line1)\n",
    "        w2v_file.write('\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500\n"
     ]
    }
   ],
   "source": [
    "print(len(w2v_X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HistGradientBoostingClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HistGradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "HistGradientBoostingClassifier()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "gradboost_classifier = HistGradientBoostingClassifier()\n",
    "w2v_df = pd.read_csv('airline_review_w2v.csv')\n",
    "gradboost_classifier.fit(w2v_df , Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(w for w in sg_w2v_model.wv.index_to_key)\n",
    "test_features_w2v = []\n",
    "for row in w2v_X_test:\n",
    "    for token in row:\n",
    "        if(token in words):\n",
    "            model_vector = (np.mean([sg_w2v_model.wv[token]], axis=0)).tolist()\n",
    "    test_features_w2v.append(model_vector)\n",
    "    if(type(model_vector) != list):\n",
    "        test_features_w2v.append(np.array([0 for i in range(size)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    }
   ],
   "source": [
    "print(len(test_features_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but HistGradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_predictions_w2v = gradboost_classifier.predict(test_features_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.375,\n",
       "  'recall': 0.13291139240506328,\n",
       "  'f1-score': 0.19626168224299062,\n",
       "  'support': 158},\n",
       " '2': {'precision': 0.07692307692307693,\n",
       "  'recall': 0.0125,\n",
       "  'f1-score': 0.021505376344086023,\n",
       "  'support': 80},\n",
       " '3': {'precision': 0.2,\n",
       "  'recall': 0.0375,\n",
       "  'f1-score': 0.0631578947368421,\n",
       "  'support': 160},\n",
       " '4': {'precision': 0.24271844660194175,\n",
       "  'recall': 0.0847457627118644,\n",
       "  'f1-score': 0.12562814070351758,\n",
       "  'support': 295},\n",
       " '5': {'precision': 0.5647149460708782,\n",
       "  'recall': 0.9083023543990086,\n",
       "  'f1-score': 0.6964370546318289,\n",
       "  'support': 807},\n",
       " 'accuracy': 0.524,\n",
       " 'macro avg': {'precision': 0.2918712939191794,\n",
       "  'recall': 0.23519190190318726,\n",
       "  'f1-score': 0.22059802973185305,\n",
       "  'support': 1500},\n",
       " 'weighted avg': {'precision': 0.4164871662537451,\n",
       "  'recall': 0.524,\n",
       "  'f1-score': 0.42794669577015854,\n",
       "  'support': 1500}}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_report = classification_report(Y_test , test_predictions_w2v , output_dict=True)\n",
    "w2v_report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
